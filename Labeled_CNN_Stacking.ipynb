{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stacked generalization with linear meta model on blobs dataset\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from keras.models import load_model\n",
    "from numpy import dstack\n",
    "from keras.utils import np_utils\n",
    "import time\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:62: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">loaded models/model_CNN_1.h5\n",
      ">loaded models/model_CNN_2.h5\n",
      ">loaded models/model_CNN_3.h5\n",
      ">loaded models/model_CNN_4.h5\n",
      ">loaded models/model_CNN_5.h5\n",
      ">loaded models/model_CNN_6.h5\n",
      ">loaded models/model_CNN_7.h5\n",
      ">loaded models/model_CNN_8.h5\n",
      ">loaded models/model_CNN_9.h5\n",
      ">loaded models/model_CNN_10.h5\n",
      ">loaded models/model_CNN_11.h5\n",
      ">loaded models/model_CNN_12.h5\n",
      ">loaded models/model_CNN_13.h5\n",
      ">loaded models/model_CNN_14.h5\n",
      "Loaded 14 models\n",
      "Model Accuracy: 0.801\n",
      "Model Accuracy: 0.801\n",
      "Model Accuracy: 0.804\n",
      "Model Accuracy: 0.815\n",
      "Model Accuracy: 0.814\n",
      "Model Accuracy: 0.815\n",
      "Model Accuracy: 0.815\n",
      "Model Accuracy: 0.568\n",
      "Model Accuracy: 0.613\n",
      "Model Accuracy: 0.630\n",
      "Model Accuracy: 0.668\n",
      "Model Accuracy: 0.623\n",
      "Model Accuracy: 0.673\n",
      "Model Accuracy: 0.679\n",
      "Stacked Test Accuracy: 0.836\n"
     ]
    }
   ],
   "source": [
    "# load models from file\n",
    "def load_all_models(n_models):\n",
    "\tall_models = list()\n",
    "\tfor i in range(n_models):\n",
    "\t\t# define filename for this ensemble\n",
    "\t\tfilename = 'models/model_CNN_' + str(i + 1) + '.h5'\n",
    "\t\t# load model from file\n",
    "\t\tmodel = load_model(filename)\n",
    "\t\t# add to list of members\n",
    "\t\tall_models.append(model)\n",
    "\t\tprint('>loaded %s' % filename)\n",
    "\treturn all_models\n",
    "\n",
    "# create stacked model input dataset as outputs from the ensemble\n",
    "def stacked_dataset(members, inputX_M,inputX_G):\n",
    "    stackX = None\n",
    "    for i in range(len(members)):\n",
    "        model=members[i]\n",
    "        if i<7:\n",
    "            # make prediction\n",
    "            yhat = model.predict(inputX_M, verbose=0)\n",
    "            # stack predictions into [rows, members, probabilities]\n",
    "            if stackX is None:\n",
    "                stackX = yhat\n",
    "            else:\n",
    "                stackX = dstack((stackX, yhat))\n",
    "        else:\n",
    "            # make prediction\n",
    "            yhat = model.predict(inputX_G, verbose=0)\n",
    "            # stack predictions into [rows, members, probabilities]\n",
    "            if stackX is None:\n",
    "                stackX = yhat\n",
    "            else:\n",
    "                stackX = dstack((stackX, yhat))            \n",
    "                \n",
    "    # flatten predictions to [rows, members x probabilities]\n",
    "    stackX = stackX.reshape((stackX.shape[0], stackX.shape[1]*stackX.shape[2]))\n",
    "    return stackX\n",
    "\n",
    "# fit a model based on the outputs from the ensemble members\n",
    "def fit_stacked_model(members, inputX, inputy):\n",
    "    # create dataset using ensemble\n",
    "    inputX_M=inputX[:, :, :, :4]\n",
    "    inputX_G=inputX[:, :, :, 4:]\n",
    "    stackedX = stacked_dataset(members, inputX_M,inputX_G)\n",
    "    # fit standalone model\n",
    "    model = LogisticRegression(max_iter=6500)\n",
    "    model.fit(stackedX, inputy)\n",
    "    return model\n",
    "\n",
    "# make a prediction with the stacked model\n",
    "def stacked_prediction(members, model, inputX):\n",
    "    # create dataset using ensemble\n",
    "    inputX_M=inputX[:, :, :, :4]\n",
    "    inputX_G=inputX[:, :, :, 4:]\n",
    "    stackedX = stacked_dataset(members, inputX_M,inputX_G)\n",
    "    # make a prediction\n",
    "    yhat = model.predict(stackedX)\n",
    "    return yhat\n",
    "\n",
    "\n",
    "start_time = time.clock()\n",
    "np.random.seed(7)\n",
    "random.seed(7)\n",
    "\n",
    "filename = 'Combined Trajectory_Label_Geolife/Revised_KerasData_Smoothing_GIS_new_8.pickle'\n",
    "\n",
    "with open(filename, mode='rb') as f:\n",
    "    TotalInput, FinalLabel = pickle.load(f, encoding='latin1')  # Also can use the encoding 'iso-8859-1'\n",
    "\n",
    "NoClass = len(list(set(np.ndarray.flatten(FinalLabel))))\n",
    "Threshold = len(TotalInput[0, 0, :, 0])\n",
    "\n",
    "# Making training and test data: 80% Training, 20% Test\n",
    "Train_X, Test_X, Train_Y, Test_Y_ori = train_test_split(TotalInput, FinalLabel, test_size=0.20, random_state=7)\n",
    "\n",
    "Test_M_X=Test_X[:, :, :, :4]\n",
    "Test_G_X=Test_X[:, :, :, 4:]\n",
    "\n",
    "\n",
    "# load all models\n",
    "n_members = 14\n",
    "members = load_all_models(n_members)\n",
    "print('Loaded %d models' % len(members))\n",
    "# evaluate standalone models on test dataset\n",
    "for i in range(len(members)):\n",
    "    model=members[i]\n",
    "    if i <7:\n",
    "        testy_enc = np_utils.to_categorical(Test_Y_ori, num_classes=NoClass)\n",
    "        _, acc = model.evaluate(Test_M_X, testy_enc, verbose=0)\n",
    "        print('Model Accuracy: %.3f' % acc)\n",
    "    else:\n",
    "        testy_enc = np_utils.to_categorical(Test_Y_ori, num_classes=NoClass)\n",
    "        _, acc = model.evaluate(Test_G_X, testy_enc, verbose=0)\n",
    "        print('Model Accuracy: %.3f' % acc)\n",
    "# fit stacked model using the ensemble\n",
    "model = fit_stacked_model(members, Test_X, Test_Y_ori)\n",
    "# evaluate model on test set\n",
    "yhat = stacked_prediction(members, model, Test_X)\n",
    "acc = accuracy_score(Test_Y_ori, yhat)\n",
    "print('Stacked Test Accuracy: %.3f' % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:  [[1988   69   56    4    5]\n",
      " [ 169  911   31    9    1]\n",
      " [ 133   31 1136  120   19]\n",
      " [  73   28  170  594   23]\n",
      " [  84    7   15   19  793]]\n",
      "Recall [0.9368520263901979, 0.8126672613737734, 0.7894371091035441, 0.668918918918919, 0.8638344226579521]\n",
      "precision [0.8124233755619126, 0.8709369024856597, 0.8068181818181818, 0.7962466487935657, 0.9429250891795482]\n",
      "0.02159000000000333 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:3: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:44: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import time\n",
    "start_time = time.clock()\n",
    "NoClass=5\n",
    "counter = 0\n",
    "for i in range(len(yhat)):\n",
    "    if yhat[i] == Test_Y_ori[i]:\n",
    "        counter += 1\n",
    "Accuracy = counter * 1./len(yhat)\n",
    "\n",
    "ActualPositive = []\n",
    "for i in range(NoClass):\n",
    "    AA = np.where(Test_Y_ori == i)[0]\n",
    "    ActualPositive.append(AA)\n",
    "\n",
    "PredictedPositive = []\n",
    "for i in range(NoClass):\n",
    "    AA = np.where(yhat == i)[0]\n",
    "    PredictedPositive.append(AA)\n",
    "\n",
    "TruePositive = []\n",
    "FalsePositive = []\n",
    "for i in range(NoClass):\n",
    "    AA = []\n",
    "    BB = []\n",
    "    for j in PredictedPositive[i]:\n",
    "        if yhat[j] == Test_Y_ori[j]:\n",
    "            AA.append(j)\n",
    "        else:\n",
    "            BB.append(j)\n",
    "    TruePositive.append(AA)\n",
    "    FalsePositive.append(BB)\n",
    "Precision = []\n",
    "Recall = []\n",
    "for i in range(NoClass):\n",
    "    Precision.append(len(TruePositive[i]) * 1./len(PredictedPositive[i]))\n",
    "    Recall.append(len(TruePositive[i]) * 1./len(ActualPositive[i]))\n",
    "\n",
    "ConfusionM = confusion_matrix(list(Test_Y_ori), yhat, labels=[0, 1, 2, 3, 4])\n",
    "\n",
    "print('Confusion Matrix: ', ConfusionM)\n",
    "print(\"Recall\", Recall)\n",
    "print('precision', Precision)\n",
    "print(time.clock() - start_time, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6386215160483345"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(Recall)/len(Recall)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6805524141188277"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(Precision)/len(Precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "F_score=[]\n",
    "for i in range(len(Precision)):\n",
    "    f=(2*Precision[i]*Recall[i])/(Precision[i]+Recall[i])\n",
    "    F_score.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6469828061229403"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(F_score)/len(F_score)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cpu.m69",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m69"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
